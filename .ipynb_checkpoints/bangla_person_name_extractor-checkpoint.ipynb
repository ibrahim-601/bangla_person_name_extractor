{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3uUrkfJAA3U"
   },
   "source": [
    "# Training Bangla Name Extractor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBaXZbSdAbFp"
   },
   "source": [
    "### Step 1: Clone git repository\n",
    "Clone the [bangla_person_name_extractor](https://github.com/ibrahim-601/bangla_person_name_extractor) repository and go to the repository folder. We are cloning at first because we're training the model on colab. If you have already cloned the repository no need to clone again. You can skip this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLNOVNu7akhC",
    "outputId": "76f8914d-f241-49f1-f35b-1678c595c331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bangla_person_name_extractor'...\n",
      "remote: Enumerating objects: 73, done.\u001b[K\n",
      "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
      "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
      "remote: Total 73 (delta 27), reused 64 (delta 18), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (73/73), 28.92 KiB | 1.45 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ibrahim-601/bangla_person_name_extractor.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8ulP2_-bddx"
   },
   "source": [
    "Go to the cloned directory. If you opened this notebook after cloning then no need to run this cell. If your terminal is one folder above the cloned directory then you can run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1W0sbPrA72-",
    "outputId": "b8eedda9-1505-4f2f-9ace-2866eee77df9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/bangla_person_name_extractor\n"
     ]
    }
   ],
   "source": [
    "%cd bangla_person_name_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcvUbJSkBg_L"
   },
   "source": [
    "### Step 2: Environment setup\n",
    "Install required packages using pip by running below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAXydKSF1kFM",
    "outputId": "45f50c94-9c2c-4b43-d1ec-c8c050d7426d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.2.2)\n",
      "Requirement already satisfied: spacy==3.5.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.5.4)\n",
      "Collecting spacy-transformers==1.2.5 (from -r requirements.txt (line 3))\n",
      "  Downloading spacy_transformers-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.8/190.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.6.6)\n",
      "Collecting gradio (from -r requirements.txt (line 5))\n",
      "  Downloading gradio-3.36.1-py3-none-any.whl (19.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (1.22.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (1.10.11)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.5.4->-r requirements.txt (line 2)) (3.3.0)\n",
      "Collecting transformers<4.31.0,>=3.4.0 (from spacy-transformers==1.2.5->-r requirements.txt (line 3))\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers==1.2.5->-r requirements.txt (line 3)) (2.0.1+cu118)\n",
      "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers==1.2.5->-r requirements.txt (line 3))\n",
      "  Downloading spacy_alignments-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 4)) (3.12.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 4)) (4.11.2)\n",
      "Collecting aiofiles (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 5)) (3.8.4)\n",
      "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 5)) (4.2.2)\n",
      "Collecting fastapi (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting gradio-client>=0.2.7 (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading gradio_client-0.2.9-py3-none-any.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 5)) (2.1.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 5)) (3.7.1)\n",
      "Collecting mdit-py-plugins<=0.3.3 (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 5)) (1.5.3)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 5)) (8.4.0)\n",
      "Collecting pydub (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 5)) (2.14.0)\n",
      "Collecting python-multipart (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 5)) (6.0)\n",
      "Collecting semantic-version (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading uvicorn-0.23.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets>=10.0 (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 5)) (0.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 5)) (4.3.3)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r requirements.txt (line 5)) (0.12.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio->-r requirements.txt (line 5)) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio->-r requirements.txt (line 5)) (4.7.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 5)) (0.1.2)\n",
      "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 5))\n",
      "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
      "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting mdit-py-plugins<=0.3.3 (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
      "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
      "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
      "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
      "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio->-r requirements.txt (line 5))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio->-r requirements.txt (line 5)) (2022.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4->-r requirements.txt (line 2)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4->-r requirements.txt (line 2)) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4->-r requirements.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.4->-r requirements.txt (line 2)) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.4->-r requirements.txt (line 2)) (0.1.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers==1.2.5->-r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers==1.2.5->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers==1.2.5->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers==1.2.5->-r requirements.txt (line 3)) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers==1.2.5->-r requirements.txt (line 3)) (16.0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers==1.2.5->-r requirements.txt (line 3)) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers==1.2.5->-r requirements.txt (line 3))\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers==1.2.5->-r requirements.txt (line 3))\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.5.4->-r requirements.txt (line 2)) (8.1.4)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio->-r requirements.txt (line 5))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 4)) (2.4.1)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio->-r requirements.txt (line 5))\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio->-r requirements.txt (line 5))\n",
      "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.4->-r requirements.txt (line 2)) (1.7.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio->-r requirements.txt (line 5)) (3.7.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r requirements.txt (line 5)) (0.19.3)\n",
      "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 5))\n",
      "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers==1.2.5->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio->-r requirements.txt (line 5)) (1.1.2)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=a6c02fbbb7cd852e02168f50b3ee3827304a4f8a6bc6dea721a0700c06ed72d2\n",
      "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: tokenizers, safetensors, pydub, ffmpy, websockets, uc-micro-py, spacy-alignments, semantic-version, python-multipart, orjson, markdown-it-py, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, transformers, httpx, fastapi, gradio-client, gradio, spacy-transformers\n",
      "  Attempting uninstall: markdown-it-py\n",
      "    Found existing installation: markdown-it-py 3.0.0\n",
      "    Uninstalling markdown-it-py-3.0.0:\n",
      "      Successfully uninstalled markdown-it-py-3.0.0\n",
      "Successfully installed aiofiles-23.1.0 fastapi-0.100.0 ffmpy-0.3.0 gradio-3.36.1 gradio-client-0.2.9 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 orjson-3.9.2 pydub-0.25.1 python-multipart-0.0.6 safetensors-0.3.1 semantic-version-2.10.0 spacy-alignments-0.9.0 spacy-transformers-1.2.5 starlette-0.27.0 tokenizers-0.13.3 transformers-4.30.2 uc-micro-py-1.0.2 uvicorn-0.23.0 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5f5CIwvBFki"
   },
   "source": [
    "### Step 3: Download and process data\n",
    "We need to call `download_data()` function from utils/downloder.py to download provided datasets. Data will be downloaded into two files inside `data_raw` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Th1Y2iseCmkn",
    "outputId": "98764e64-75da-4e47-c1c8-5ea8e8ca7f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data.\n"
     ]
    }
   ],
   "source": [
    "from bangla_person_ner.utils import downloader\n",
    "\n",
    "# download the dataset provided for the project\n",
    "downloader.download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygK3YNUeEboN"
   },
   "source": [
    "After downloading the data, we will clean and reformat the data. To do so, we call `process_text_data()` and `process_jsonl_data()` functions from `bangla_person_ner/preprocessing/raw_data_processing.py` for dataset_1 and dataset_2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9a6OcdDFGnn",
    "outputId": "93fccb5b-90ad-4e9e-ebec-c99896a53239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data summary:  data_1\n",
      "------------------------------\n",
      "Total sentence : 6580\n",
      "Sentence with person tag: 1776\n",
      "Sentence without person tag: 4804\n",
      "\n",
      "Data summary:  data_2\n",
      "------------------------------\n",
      "Total sentence : 3494\n",
      "Sentence with person tag: 1189\n",
      "Sentence without person tag: 2305\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bangla_person_ner.config.config as cfg\n",
    "from bangla_person_ner.preprocessing.raw_data_processing import process_text_data, process_jsonl_data\n",
    "\n",
    "# process text data (data_1)\n",
    "data_1 = process_text_data(data_path=cfg.RAW_DATA1_FILE_PATH, save_path=cfg.PROCESSESED_DATA1_PATH)\n",
    "\n",
    "# process jsonl data (data_2)\n",
    "data_2 = process_jsonl_data(data_path=cfg.RAW_DATA2_FILE_PATH, save_path=cfg.PROCESSESED_DATA2_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcKbXEtZF65B"
   },
   "source": [
    "### Step 4: Split data and convert to Spacy format\n",
    "Now we will split the dataset into train, validation, and test set. Then we will convert them into spacy binary data and store them. All of this can be done by calling `split_and_convert_data()` from `bangla_person_ner/preprocessing/train_data_processing.py` and passing processed data from previous step to this function. Path for the saved data can be obtained from `config/config.py` file. Path for train, validation, and test is defined as vairable `TAIN_DATA_PATH`, `VALID_DATA_PATH`, and `TEST_DATA_PATH` respectively in `config.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXh648KNGR_6",
    "outputId": "a42d3a35-2c84-41d1-cdfb-c01dd37a5c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training data summary :  Data with PERSON tag\n",
      "------------------------------\n",
      "Number of train samples :  2372\n",
      "Number of validation samples :  297\n",
      "Number of test samples :  296\n",
      "-----------------------------------\n",
      "Number of total data :  2965\n",
      "\n",
      "Training data summary :  Data without PERSON tag\n",
      "------------------------------\n",
      "Number of train samples :  5687\n",
      "Number of validation samples :  711\n",
      "Number of test samples :  711\n",
      "-----------------------------------\n",
      "Number of total data :  7109\n",
      "\n",
      "Training data summary :  All data\n",
      "------------------------------\n",
      "Number of train samples :  8059\n",
      "Number of validation samples :  1008\n",
      "Number of test samples :  1007\n",
      "-----------------------------------\n",
      "Number of total data :  10074\n",
      "\n",
      "Saving spacy binary format data...\n",
      "Saved train data at :  /content/bangla_person_name_extractor/dataset/train.spacy\n",
      "Saved train data at :  /content/bangla_person_name_extractor/dataset/valid.spacy\n",
      "Saved train data at :  /content/bangla_person_name_extractor/dataset/test.spacy\n"
     ]
    }
   ],
   "source": [
    "from bangla_person_ner.preprocessing.train_data_processing import split_and_convert_data\n",
    "\n",
    "# this function accepts tuple of data_1 and data_2\n",
    "tuple_data = (data_1, data_2)\n",
    "split_and_convert_data(tuple_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLl-Drz6SUX8"
   },
   "source": [
    "### Step 5: Generate training config of Spacy\n",
    "We need to generate config file for model training with spacy. We can do so by running following cell. It contains Spacy CLI command to generate training configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fT3Hllm62Tlo",
    "outputId": "4edb9856-03a3-4fd2-d444-28cdb2304ab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-16 07:36:57.923817: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-16 07:36:58.886794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-16 07:37:00.226467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-16 07:37:00.226947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-16 07:37:00.227137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: bn\n",
      "- Pipeline: ner\n",
      "- Optimize for: accuracy\n",
      "- Hardware: GPU\n",
      "- Transformer: sagorsarker/bangla-bert-base\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config/spacy_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train spacy_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config bangla_person_ner/config/spacy_config.cfg --lang bn --pipeline ner --optimize accuracy --gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoJcbtwmXzSm"
   },
   "source": [
    "We would change some parameter in the config file.\n",
    "1. By default spacy sets transformer model to `sagorsarker/bangla-bert-base`. We will change it to `csebuetnlp/banglabert`.\n",
    "2. We will set `max_epochs` to 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZKEY05YYpVR"
   },
   "source": [
    "### Step 6: Train the model\n",
    "Now we will train the model by running following cell. It contains Spacy CLI command to for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QviwedJ32tb",
    "outputId": "db57b024-7217-4fcd-c28a-b5355d897a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-16 07:38:51.496357: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-16 07:38:52.480000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-16 07:38:53.857585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-16 07:38:53.858041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-16 07:38:53.858217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "\u001b[38;5;2m✔ Created output directory: models\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: models\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-07-16 07:38:59,656] [INFO] Set up nlp object from config\n",
      "[2023-07-16 07:38:59,672] [INFO] Pipeline: ['transformer', 'ner']\n",
      "[2023-07-16 07:38:59,676] [INFO] Created vocabulary\n",
      "[2023-07-16 07:38:59,676] [INFO] Finished initializing nlp object\n",
      "Downloading (…)okenizer_config.json: 100% 119/119 [00:00<00:00, 693kB/s]\n",
      "Downloading (…)lve/main/config.json: 100% 586/586 [00:00<00:00, 3.44MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100% 528k/528k [00:00<00:00, 1.22MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 678kB/s]\n",
      "Downloading pytorch_model.bin: 100% 443M/443M [00:01<00:00, 374MB/s]\n",
      "Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2023-07-16 07:39:33,174] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0         399.71    261.85    5.19    3.01   18.95    0.05\n",
      "  1     200       27049.41  31547.94   75.87   74.61   77.17    0.76\n",
      "  3     400        2033.39   2976.52   80.27   78.43   82.19    0.80\n",
      "  5     600        1415.52   1928.16   80.48   77.04   84.25    0.80\n",
      "  7     800        1056.87   1453.52   78.61   82.99   74.66    0.79\n",
      "  8    1000         963.10   1143.85   81.60   81.69   81.51    0.82\n",
      " 10    1200         792.82    968.75   81.35   80.09   82.65    0.81\n",
      " 12    1400         746.50    942.26   78.61   82.46   75.11    0.79\n",
      " 14    1600         860.16    870.72   80.14   82.06   78.31    0.80\n",
      " 16    1800         688.07    830.35   81.55   79.01   84.25    0.82\n",
      " 17    2000         636.21    801.00   82.55   80.92   84.25    0.83\n",
      " 19    2200         595.23    734.16   80.54   78.95   82.19    0.81\n",
      " 21    2400         549.03    700.88   81.87   79.83   84.02    0.82\n",
      " 23    2600         557.37    724.42   81.79   78.29   85.62    0.82\n",
      " 24    2800         572.97    706.72   79.67   81.58   77.85    0.80\n",
      " 26    3000         545.62    691.81   76.92   78.57   75.34    0.77\n",
      " 28    3200         546.27    694.27   82.69   79.96   85.62    0.83\n",
      " 30    3400         534.34    677.52   80.32   79.60   81.05    0.80\n",
      " 32    3600         524.66    673.09   80.46   81.02   79.91    0.80\n",
      " 33    3800         566.77    712.92   81.34   82.09   80.59    0.81\n",
      " 35    4000         546.90    675.28   79.95   80.70   79.22    0.80\n",
      " 37    4200         496.09    643.59   81.78   79.65   84.02    0.82\n",
      " 39    4400         729.00    663.15   80.72   79.73   81.74    0.81\n",
      " 40    4600        2917.95    713.26   80.59   80.14   81.05    0.81\n",
      " 42    4800         497.26    647.73   82.14   80.35   84.02    0.82\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "models/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train bangla_person_ner/config/spacy_config.cfg --output bangla_person_ner/models --gpu-id 0 --paths.train bangla_person_ner/dataset/train.spacy --paths.dev bangla_person_ner/dataset/valid.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohdUCNnMjgjv"
   },
   "source": [
    "### Step 7: Evaluating model\n",
    "In this step we will evaluate the trained model using Spacy CLI. Spacy saves two models - `model-best`, and `model-last`. We will use model-best for evaluation and further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77utZyar8MNO",
    "outputId": "14ce20aa-37a4-41e6-90e2-7ec396d1b09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-16 08:29:35.776537: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-16 08:29:36.775471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-16 08:29:38.110313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-16 08:29:38.110754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-16 08:29:38.110939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     -    \n",
      "NER P   82.99\n",
      "NER R   85.08\n",
      "NER F   84.02\n",
      "SPEED   3236 \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "             P       R       F\n",
      "PERSON   82.99   85.08   84.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy benchmark accuracy bangla_person_ner/models/model-best test.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vf2vR9Us-rLw"
   },
   "source": [
    "### Step 8: Make predictions\n",
    "We will import `BanglaPersorNer` from `bangla_person_ner/bangla_person_ner.py` to make predictions. This class contains necessary codes to extract bangla person name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "z33Lb0EN5StF"
   },
   "outputs": [],
   "source": [
    "from bangla_person_ner.bangla_person_ner import BanglaPersorNer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TIVcOCS_Jcv"
   },
   "source": [
    "We defined `texts` variable with 4 bangla texts. Two of them contains person name and the other two does not. We will create an object of `BanglaPersonNer`. For each item of `texts` we call `extract_person_name()` function of `BanglaPersorNer` class, pass the item to the function, and print the returned value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9aXk02YuKQhR",
    "outputId": "80cde3b6-ad88-46c6-b540-7d57840afaad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'মো. আলমের কাছ থেকে ১৫ লাখ টাকা আদায় করা হয়।', 'extracted_names': [{'name': 'মো. আলমের', 'label': 'PERSON', 'start': 0, 'end': 2}]}\n",
      "{'sentence': 'ডা. মো. শরিফুল ইসলাম, শহীদ সোহরাওয়ার্দী মেডিকেল, কলেজ ও হাসপাতাল।', 'extracted_names': [{'name': 'ডা.', 'label': 'PERSON', 'start': 0, 'end': 1}, {'name': 'মো. শরিফুল ইসলাম', 'label': 'PERSON', 'start': 1, 'end': 4}]}\n",
      "{'sentence': \"এ ট্যাবলেটটির নাম হতে পারে 'আইপ্যাড ম্যাক্সি'।\", 'extracted_names': 'কোন নাম খুঁজে পাওয়া যায় নি/No name is found'}\n",
      "{'sentence': 'এতিমখানার কর্মকর্তা-শিক্ষার্থীরা কমিটি ও চুক্তির বিরুদ্ধে আন্দোলন শুরু করে।', 'extracted_names': 'কোন নাম খুঁজে পাওয়া যায় নি/No name is found'}\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"মো. আলমের কাছ থেকে ১৫ লাখ টাকা আদায় করা হয়।\",\n",
    "    \"ডা. মো. শরিফুল ইসলাম, শহীদ সোহরাওয়ার্দী মেডিকেল, কলেজ ও হাসপাতাল।\",\n",
    "    \"এ ট্যাবলেটটির নাম হতে পারে 'আইপ্যাড ম্যাক্সি'।\",\n",
    "    \"এতিমখানার কর্মকর্তা-শিক্ষার্থীরা কমিটি ও চুক্তির বিরুদ্ধে আন্দোলন শুরু করে।\",\n",
    "]\n",
    "bpne = BanglaPersorNer()\n",
    "for text in texts:\n",
    "  res = bpne.extract_person_name(text)\n",
    "  print(res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
