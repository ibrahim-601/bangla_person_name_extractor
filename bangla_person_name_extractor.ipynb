{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Bangla Name Extractor Model"
      ],
      "metadata": {
        "id": "P3uUrkfJAA3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Clone git repository\n",
        "Clone the [bangla_person_name_extractor](https://github.com/ibrahim-601/bangla_person_name_extractor) repository and go to the repository folder. We are cloning at first because we're training the model on colab. If you have already cloned the repository no need to clone again. You can skip this step"
      ],
      "metadata": {
        "id": "rBaXZbSdAbFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ibrahim-601/bangla_person_name_extractor.git"
      ],
      "metadata": {
        "id": "fLNOVNu7akhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f8914d-f241-49f1-f35b-1678c595c331"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bangla_person_name_extractor'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 73 (delta 27), reused 64 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), 28.92 KiB | 1.45 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to the cloned directory. If you opened this notebook after cloning then no need to run this cell. If your terminal is one folder above the cloned directory then you can run this cell."
      ],
      "metadata": {
        "id": "X8ulP2_-bddx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd bangla_person_name_extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1W0sbPrA72-",
        "outputId": "e13a7cc0-8c26-4845-cc4f-bdd179033119"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bangla_person_name_extractor/bangla_person_name_extractor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Environment setup\n",
        "Install required packages using pip by running below cell."
      ],
      "metadata": {
        "id": "VcvUbJSkBg_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ZAXydKSF1kFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ced3930-ec07-41dd-a195-cee814e91eeb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.5.4)\n",
            "Collecting spacy-transformers (from -r requirements.txt (line 3))\n",
            "  Downloading spacy_transformers-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.8/190.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.6.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (1.10.11)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 2)) (3.3.0)\n",
            "Collecting transformers<4.31.0,>=3.4.0 (from spacy-transformers->-r requirements.txt (line 3))\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy-transformers->-r requirements.txt (line 3)) (2.0.1+cu118)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy-transformers->-r requirements.txt (line 3))\n",
            "  Downloading spacy_alignments-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 4)) (3.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 4)) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy->-r requirements.txt (line 2)) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 2)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 2)) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 2)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->-r requirements.txt (line 2)) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->-r requirements.txt (line 2)) (0.1.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers->-r requirements.txt (line 3)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers->-r requirements.txt (line 3)) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy-transformers->-r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers->-r requirements.txt (line 3)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers->-r requirements.txt (line 3)) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers->-r requirements.txt (line 3))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers->-r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers->-r requirements.txt (line 3)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers->-r requirements.txt (line 3))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<4.31.0,>=3.4.0->spacy-transformers->-r requirements.txt (line 3))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy->-r requirements.txt (line 2)) (8.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 4)) (2.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->-r requirements.txt (line 2)) (2.1.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 2)) (1.7.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.31.0,>=3.4.0->spacy-transformers->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy-transformers->-r requirements.txt (line 3)) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, spacy-alignments, huggingface-hub, transformers, spacy-transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 spacy-alignments-0.9.0 spacy-transformers-1.2.5 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Download and process data\n",
        "We need to call `download_data()` function from utils/downloder.py to download provided datasets. Data will be downloaded into two files inside `data_raw` directory."
      ],
      "metadata": {
        "id": "I5f5CIwvBFki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import downloader\n",
        "\n",
        "# download the dataset provided for the project\n",
        "downloader.download_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th1Y2iseCmkn",
        "outputId": "be7e807e-62e6-46d2-8e55-876ebd6b8bd9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After downloading the data, we will clean and reformat the data. To do so, we call `process_text_data()` and `process_jsonl_data()` functions from `preprocessing/raw_data_processing.py` for dataset_1 and dataset_2 respectively."
      ],
      "metadata": {
        "id": "ygK3YNUeEboN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import config.config as cfg\n",
        "from preprocessing.raw_data_processing import process_text_data, process_jsonl_data\n",
        "\n",
        "# process text data (data_1)\n",
        "data_1_path = os.path.join(cfg.RAW_DATA_DOWNLOAD_DIR, cfg.RAW_DATA_1_FILE_NAME)\n",
        "save_data_1_path = os.path.join(cfg.PROCESSESED_DATA_SAVE_DIR, cfg.PROCESSESED_DATA_1_NAME)\n",
        "data_1 = process_text_data(data_path=data_1_path, save_path=save_data_1_path)\n",
        "\n",
        "# process jsonl data (data_2)\n",
        "data_2_path_ = os.path.join(cfg.RAW_DATA_DOWNLOAD_DIR, cfg.RAW_DATA_2_FILE_NAME)\n",
        "save_data_2_path = os.path.join(cfg.PROCESSESED_DATA_SAVE_DIR, cfg.PROCESSESED_DATA_2_NAME)\n",
        "data_2 = process_jsonl_data(data_path=data_2_path_, save_path=save_data_2_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9a6OcdDFGnn",
        "outputId": "3fb36eaa-c587-4425-980e-388a2d146192"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data summary:  data_1\n",
            "------------------------------\n",
            "Total sentence : 6580\n",
            "Sentence with person tag: 1776\n",
            "Sentence without person tag: 4804\n",
            "\n",
            "Data summary:  data_2\n",
            "------------------------------\n",
            "Total sentence : 3494\n",
            "Sentence with person tag: 1189\n",
            "Sentence without person tag: 2305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Split data and convert to Spacy format\n",
        "Now we will split the dataset into train, validation, and test set. Then we will convert them into spacy binary data and store them. All of this can be done by calling `split_and_convert_data()` from `preprocessing/train_data_processing.py` and passing processed data from previous step to this function. Path for the saved data can be obtained from `config/config.py` file. Path for train, validation, and test is defined as vairable `TAIN_DATA_PATH`, `VALID_DATA_PATH`, and `TEST_DATA_PATH` respectively in `config.py` file."
      ],
      "metadata": {
        "id": "mcKbXEtZF65B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing.train_data_processing import split_and_convert_data\n",
        "\n",
        "# this function accepts tuple of data_1 and data_2\n",
        "tuple_data = (data_1, data_2)\n",
        "split_and_convert_data(tuple_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXh648KNGR_6",
        "outputId": "3896bcd9-cf12-4aaf-df0c-6f31c33b1ec6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training data summary :  Data with PERSON tag\n",
            "------------------------------\n",
            "Number of train samples :  2372\n",
            "Number of validation samples :  297\n",
            "Number of test samples :  296\n",
            "-----------------------------------\n",
            "Number of total data :  2965\n",
            "\n",
            "Training data summary :  Data without PERSON tag\n",
            "------------------------------\n",
            "Number of train samples :  5687\n",
            "Number of validation samples :  711\n",
            "Number of test samples :  711\n",
            "-----------------------------------\n",
            "Number of total data :  7109\n",
            "\n",
            "Training data summary :  All data\n",
            "------------------------------\n",
            "Number of train samples :  8059\n",
            "Number of validation samples :  1008\n",
            "Number of test samples :  1007\n",
            "-----------------------------------\n",
            "Number of total data :  10074\n",
            "Saving spacy binary format data...\n",
            "Saved train data at :  /content/bangla_person_name_extractor/dataset/train.spacy\n",
            "Saved train data at :  /content/bangla_person_name_extractor/dataset/valid.spacy\n",
            "Saved train data at :  /content/bangla_person_name_extractor/dataset/test.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Generate training config of Spacy\n",
        "We need to generate config file for model training with spacy. We can do so by running following cell. It contains Spacy CLI command to generate training configuration file."
      ],
      "metadata": {
        "id": "yLl-Drz6SUX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init config config/spacy_config.cfg --lang bn --pipeline ner --optimize accuracy --gpu"
      ],
      "metadata": {
        "id": "fT3Hllm62Tlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f17f28c-78de-4576-936d-b46c4a9461f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-14 17:49:42.808719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: bn\n",
            "- Pipeline: ner\n",
            "- Optimize for: accuracy\n",
            "- Hardware: GPU\n",
            "- Transformer: sagorsarker/bangla-bert-base\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config/spacy_config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train spacy_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would change some parameter in the config file.\n",
        "1. By default spacy sets transformer model to `sagorsarker/bangla-bert-base`. We will change it to `csebuetnlp/banglabert`.\n",
        "2. We will set `max_epochs` to 50."
      ],
      "metadata": {
        "id": "HoJcbtwmXzSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Train the model\n",
        "Now we will train the model by running following cell. It contains Spacy CLI command to for training."
      ],
      "metadata": {
        "id": "xZKEY05YYpVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config/spacy_config.cfg --output models --gpu-id 0 --paths.train dataset/train.spacy --paths.dev dataset/valid.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QviwedJ32tb",
        "outputId": "02a29f07-e310-4f2d-902f-53e3515824f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-15 09:59:23.799067: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Created output directory: models\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: models\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-07-15 09:59:32,121] [INFO] Set up nlp object from config\n",
            "[2023-07-15 09:59:32,150] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2023-07-15 09:59:32,156] [INFO] Created vocabulary\n",
            "[2023-07-15 09:59:32,156] [INFO] Finished initializing nlp object\n",
            "Downloading (…)okenizer_config.json: 100% 119/119 [00:00<00:00, 721kB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 586/586 [00:00<00:00, 3.82MB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 528k/528k [00:00<00:00, 3.23MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 112/112 [00:00<00:00, 748kB/s]\n",
            "Downloading pytorch_model.bin: 100% 443M/443M [00:01<00:00, 288MB/s]\n",
            "Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
            "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-07-15 10:00:04,956] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0         385.44    383.33    4.77    2.75   17.95    0.05\n",
            "  1     200       27909.36  32110.19   75.84   80.95   71.33    0.76\n",
            "  3     400        2175.08   3171.97   81.37   82.34   80.42    0.81\n",
            "  5     600        1475.60   2002.22   74.50   85.98   65.73    0.75\n",
            "  6     800        1084.95   1441.13   80.72   83.00   78.55    0.81\n",
            "  8    1000         786.82   1055.77   78.92   83.20   75.06    0.79\n",
            " 10    1200         795.24    994.49   79.61   83.04   76.46    0.80\n",
            " 12    1400         686.22    891.92   79.50   85.11   74.59    0.80\n",
            " 13    1600         659.62    837.12   81.53   83.95   79.25    0.82\n",
            " 15    1800         656.77    841.32   77.37   84.53   71.33    0.77\n",
            " 17    2000         552.65    733.88   81.39   83.91   79.02    0.81\n",
            " 19    2200         585.35    751.92   81.53   83.95   79.25    0.82\n",
            " 21    2400         555.35    715.07   81.55   82.23   80.89    0.82\n",
            " 22    2600         561.54    707.42   77.78   84.85   71.79    0.78\n",
            " 24    2800         565.51    736.04   81.58   81.11   82.05    0.82\n",
            " 26    3000         697.20    690.50   80.91   82.89   79.02    0.81\n",
            " 28    3200         559.84    723.61   79.86   82.22   77.62    0.80\n",
            " 29    3400        1398.48    697.28   78.73   82.78   75.06    0.79\n",
            " 31    3600        4833.85    779.73   77.94   82.17   74.13    0.78\n",
            " 33    3800        1531.05    698.56   78.68   82.95   74.83    0.79\n",
            " 35    4000         537.03    676.04   76.92   78.12   75.76    0.77\n",
            " 36    4200         482.54    619.27   79.90   81.55   78.32    0.80\n",
            " 38    4400        4967.99    711.69   80.48   82.24   78.79    0.80\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "models/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Evaluating model\n",
        "In this step we will evaluate the trained model using Spacy CLI. Spacy saves two models - `model-best`, and `model-last`. We will use model-best for evaluation and further usage."
      ],
      "metadata": {
        "id": "ohdUCNnMjgjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy benchmark accuracy models/model-best dataset/test.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77utZyar8MNO",
        "outputId": "9200c1c5-d8ab-4bdf-c78d-e14355765ed1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-15 10:42:46.277159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   80.40\n",
            "NER R   84.15\n",
            "NER F   82.23\n",
            "SPEED   3140 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "          P       R       F\n",
            "PER   80.40   84.15   82.23\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Make predictions\n",
        "We have `extract_person_name()` function in `bangla_person_name_extractor.py` to make predictions. We will import that in the next cell."
      ],
      "metadata": {
        "id": "vf2vR9Us-rLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bangla_person_name_extractor import extract_person_name"
      ],
      "metadata": {
        "id": "z33Lb0EN5StF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We defined `texts` variable with 4 bangla texts. Two of them contains person name and the rest does not. We will iterate over each item of `texts` and call `extract_person_name()` by passing the item and print the returned value."
      ],
      "metadata": {
        "id": "2TIVcOCS_Jcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"এ ট্যাবলেটটির নাম হতে পারে 'আইপ্যাড ম্যাক্সি'।\",\n",
        "    \"মো. আলমের কাছ থেকে ১৫ লাখ টাকা আদায় করা হয়।\",\n",
        "    \"এতিমখানার কর্মকর্তা-শিক্ষার্থীরা কমিটি ও চুক্তির বিরুদ্ধে আন্দোলন শুরু করে।\",\n",
        "    \"ডা. মো. শরিফুল ইসলাম, শহীদ সোহরাওয়ার্দী মেডিকেল, কলেজ ও হাসপাতাল।\"\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "  res = extract_person_name(text)\n",
        "  print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aXk02YuKQhR",
        "outputId": "38822ab8-7f0e-4ea4-ee42-6ddceeb7c45b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence': \"এ ট্যাবলেটটির নাম হতে পারে 'আইপ্যাড ম্যাক্সি'।\", 'extracted_names': 'কোন নাম খুঁজে পাওয়া যায় নি/No name is found'}\n",
            "{'sentence': 'মো. আলমের কাছ থেকে ১৫ লাখ টাকা আদায় করা হয়।', 'extracted_names': [{'name': 'মো. আলমের', 'label': 'PER', 'start': 0, 'end': 2}]}\n",
            "{'sentence': 'এতিমখানার কর্মকর্তা-শিক্ষার্থীরা কমিটি ও চুক্তির বিরুদ্ধে আন্দোলন শুরু করে।', 'extracted_names': 'কোন নাম খুঁজে পাওয়া যায় নি/No name is found'}\n",
            "{'sentence': 'ডা. মো. শরিফুল ইসলাম, শহীদ সোহরাওয়ার্দী মেডিকেল, কলেজ ও হাসপাতাল।', 'extracted_names': [{'name': 'ডা. মো.', 'label': 'PER', 'start': 0, 'end': 2}, {'name': 'শরিফুল ইসলাম', 'label': 'PER', 'start': 2, 'end': 4}]}\n"
          ]
        }
      ]
    }
  ]
}