{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Bangla Name Extractor Model"
      ],
      "metadata": {
        "id": "P3uUrkfJAA3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Clone git repository\n",
        "Clone the [bangla_person_name_extractor](https://github.com/ibrahim-601/bangla_person_name_extractor) repository and go to the repository folder. We are cloning at first because we're training the model on colab. If you have already cloned the repository no need to clone again. You can skip this step"
      ],
      "metadata": {
        "id": "rBaXZbSdAbFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ibrahim-601/bangla_person_name_extractor.git"
      ],
      "metadata": {
        "id": "fLNOVNu7akhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to the cloned directory. If you opened this notebook after cloning then no need to run this cell. If your terminal is one folder above the cloned directory then you can run this cell."
      ],
      "metadata": {
        "id": "X8ulP2_-bddx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd bangla_person_name_extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1W0sbPrA72-",
        "outputId": "a7e56929-8fc7-4a07-8ec0-88e3e51ed10b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bangla_person_name_extractor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Environment setup\n",
        "Install required packages using pip by running below cell."
      ],
      "metadata": {
        "id": "VcvUbJSkBg_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ZAXydKSF1kFM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Download and process data\n",
        "We need to call `download_data()` function from utils/downloder.py to download provided datasets. Data will be downloaded into two files inside `data_raw` directory."
      ],
      "metadata": {
        "id": "I5f5CIwvBFki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import downloader\n",
        "\n",
        "# download the dataset provided for the project\n",
        "downloader.download_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th1Y2iseCmkn",
        "outputId": "0e01ff0e-eb87-4a6c-bdc1-69e112c60dd8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After downloading the data, we will clean and reformat the data. To do so, we call `process_text_data()` and `process_jsonl_data()` functions from `preprocessing/raw_data_processing.py` for dataset_1 and dataset_2 respectively."
      ],
      "metadata": {
        "id": "ygK3YNUeEboN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import config.config as cfg\n",
        "from preprocessing.raw_data_processing import process_text_data, process_jsonl_data\n",
        "\n",
        "# process text data (data_1)\n",
        "data_1_path = os.path.join(cfg.RAW_DATA_DOWNLOAD_DIR, cfg.RAW_DATA_1_FILE_NAME)\n",
        "save_data_1_path = os.path.join(cfg.PROCESSESED_DATA_SAVE_DIR, cfg.PROCESSESED_DATA_1_NAME)\n",
        "data_1 = process_text_data(data_path=data_1_path, save_path=save_data_1_path)\n",
        "\n",
        "# process jsonl data (data_2)\n",
        "data_2_path_ = os.path.join(cfg.RAW_DATA_DOWNLOAD_DIR, cfg.RAW_DATA_2_FILE_NAME)\n",
        "save_data_2_path = os.path.join(cfg.PROCESSESED_DATA_SAVE_DIR, cfg.PROCESSESED_DATA_2_NAME)\n",
        "data_2 = process_jsonl_data(data_path=data_2_path_, save_path=save_data_2_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9a6OcdDFGnn",
        "outputId": "75f52059-2f81-48fd-da59-2fa830f9d442"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data summary:  data_1\n",
            "------------------------------\n",
            "Total sentence : 6580\n",
            "Sentence with person tag: 1776\n",
            "Sentence without person tag: 4804\n",
            "\n",
            "Data summary:  data_2\n",
            "------------------------------\n",
            "Total sentence : 3494\n",
            "Sentence with person tag: 1189\n",
            "Sentence without person tag: 2305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Split data and convert to Spacy format\n",
        "Now we will split the dataset into train, validation, and test set. Then we will convert them into spacy binary data and store them. All of this can be done by calling `split_and_convert_data()` from `preprocessing/train_data_processing.py` and passing processed data from previous step to this function. Path for the saved data can be obtained from `config/config.py` file. Path for train, validation, and test is defined as vairable `TAIN_DATA_PATH`, `VALID_DATA_PATH`, and `TEST_DATA_PATH` respectively in `config.py` file."
      ],
      "metadata": {
        "id": "mcKbXEtZF65B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing.train_data_processing import split_and_convert_data\n",
        "\n",
        "# this function accepts tuple of data_1 and data_2\n",
        "tuple_data = (data_1, data_2)\n",
        "split_and_convert_data(tuple_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXh648KNGR_6",
        "outputId": "f082535e-dd43-40ad-bb1a-a541457bb16f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving spacy binary format data...\n",
            "\n",
            "Training data summary :  Data with PERSON tag\n",
            "------------------------------\n",
            "Number of train samples :  2372\n",
            "Number of validation samples :  296\n",
            "Number of test samples :  297\n",
            "-----------------------------------\n",
            "Number of total data :  2965\n",
            "\n",
            "Training data summary :  Data without PERSON tag\n",
            "------------------------------\n",
            "Number of train samples :  5687\n",
            "Number of validation samples :  711\n",
            "Number of test samples :  711\n",
            "-----------------------------------\n",
            "Number of total data :  7109\n",
            "\n",
            "Training data summary :  All data\n",
            "------------------------------\n",
            "Number of train samples :  8059\n",
            "Number of validation samples :  1007\n",
            "Number of test samples :  1008\n",
            "-----------------------------------\n",
            "Number of total data :  10074\n",
            "Saved train data at :  dataset/train.spacy\n",
            "Saved train data at :  dataset/valid.spacy\n",
            "Saved train data at :  dataset/test.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Generate training config of Spacy\n",
        "We need to generate config file for model training with spacy. We can do so by running following cell. It contains Spacy CLI command to generate training configuration file."
      ],
      "metadata": {
        "id": "yLl-Drz6SUX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init config config/spacy_config.cfg --lang bn --pipeline ner --optimize accuracy --gpu"
      ],
      "metadata": {
        "id": "fT3Hllm62Tlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f17f28c-78de-4576-936d-b46c4a9461f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-14 17:49:42.808719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: bn\n",
            "- Pipeline: ner\n",
            "- Optimize for: accuracy\n",
            "- Hardware: GPU\n",
            "- Transformer: sagorsarker/bangla-bert-base\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config/spacy_config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train spacy_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would change some parameter in the config file.\n",
        "1. By default spacy sets transformer model to `sagorsarker/bangla-bert-base`. We will change it to `csebuetnlp/banglabert`.\n",
        "2. We will set `max_epochs` to 50."
      ],
      "metadata": {
        "id": "HoJcbtwmXzSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Train the model\n",
        "Now we will train the model by running following cell. It contains Spacy CLI command to for training."
      ],
      "metadata": {
        "id": "xZKEY05YYpVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config/spacy_config.cfg --output models --gpu-id 0 --paths.train dataset/train.spacy --paths.dev dataset/valid.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QviwedJ32tb",
        "outputId": "dbdf82be-bfde-4879-f4b5-0e1b2e142e1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-14 19:52:09.964047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Created output directory: models\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: models\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-07-14 19:52:14,334] [INFO] Set up nlp object from config\n",
            "[2023-07-14 19:52:14,364] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2023-07-14 19:52:14,370] [INFO] Created vocabulary\n",
            "[2023-07-14 19:52:14,370] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
            "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-07-14 19:52:25,390] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0         587.01    593.18    4.79    2.76   18.40    0.05\n",
            "  1     200       26281.70  32011.36   77.38   71.37   84.50    0.77\n",
            "  3     400        2223.94   3141.52   79.67   76.56   83.05    0.80\n",
            "  5     600        1428.68   2004.84   81.70   77.85   85.96    0.82\n",
            "  6     800         997.29   1382.39   81.14   79.53   82.81    0.81\n",
            "  8    1000         891.84   1163.46   82.22   82.13   82.32    0.82\n",
            " 10    1200        1291.64   1018.31   80.83   76.86   85.23    0.81\n",
            " 12    1400         777.00    926.84   80.29   80.68   79.90    0.80\n",
            " 13    1600         661.31    838.24   80.66   78.18   83.29    0.81\n",
            " 15    1800         673.54    833.33   79.56   80.95   78.21    0.80\n",
            " 17    2000         615.24    781.68   80.38   78.97   81.84    0.80\n",
            " 19    2200         582.29    730.80   81.28   79.58   83.05    0.81\n",
            " 20    2400         604.16    755.47   81.20   80.33   82.08    0.81\n",
            " 22    2600         543.64    718.53   78.95   77.57   80.39    0.79\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "models/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Evaluating model\n",
        "In this step we will evaluate the trained model using Spacy CLI. Spacy saves two models - `model-best`, and `model-last`. We will use model-best for evaluation and further usage."
      ],
      "metadata": {
        "id": "ohdUCNnMjgjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy benchmark accuracy models/model-best dataset/test.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77utZyar8MNO",
        "outputId": "5341fa55-bfb5-4e95-b29b-9596cdbd4925"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-14 20:18:13.289824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   80.36\n",
            "NER R   80.56\n",
            "NER F   80.46\n",
            "SPEED   3255 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "          P       R       F\n",
            "PER   80.36   80.56   80.46\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rename the `model-best` to `bangla_person_ner` as model path `config.py` is set to be like that. The piece of code which is used to make predictions also receives the same from `config.py` file."
      ],
      "metadata": {
        "id": "1cNpCiym9mH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/bangla_person_name_extractor/models/model-best models/bn_person_ner"
      ],
      "metadata": {
        "id": "2SzAZoTw82My"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Make predictions\n",
        "We have `extract_person_name()` function in `bangla_person_name_extractor.py` to make predictions. We will import that in the next cell."
      ],
      "metadata": {
        "id": "vf2vR9Us-rLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bangla_person_name_extractor as bpne"
      ],
      "metadata": {
        "id": "z33Lb0EN5StF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We defined `texts` variable with 4 bangla texts. Two of them contains person name and the rest does not. We will iterate over each item of `texts` and call `extract_person_name()` by passing the item and print the returned value."
      ],
      "metadata": {
        "id": "2TIVcOCS_Jcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"এ ট্যাবলেটটির নাম হতে পারে 'আইপ্যাড ম্যাক্সি'।\",\n",
        "    \"মো. আলমের কাছ থেকে ১৫ লাখ টাকা আদায় করা হয়।\",\n",
        "    \"এতিমখানার কর্মকর্তা-শিক্ষার্থীরা কমিটি ও চুক্তির বিরুদ্ধে আন্দোলন শুরু করে।\",\n",
        "    \"ডা. মো. শরিফুল ইসলাম, শহীদ সোহরাওয়ার্দী মেডিকেল, কলেজ ও হাসপাতাল।\"\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "  res = bpne.extract_person_name(text)\n",
        "  print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aXk02YuKQhR",
        "outputId": "bae49e42-3679-4dba-97a2-c6367cc49633"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence': \"এ ট্যাবলেটটির নাম হতে পারে 'আইপ্যাড ম্যাক্সি'।\", 'extracted_names': 'কোন নাম খুঁজে পাওয়া যায় নি/No name is found'}\n",
            "{'sentence': 'মো. আলমের কাছ থেকে ১৫ লাখ টাকা আদায় করা হয়।', 'extracted_names': [{'name': 'মো. আলমের', 'label': 'PER', 'start': 0, 'end': 2}]}\n",
            "{'sentence': 'এতিমখানার কর্মকর্তা-শিক্ষার্থীরা কমিটি ও চুক্তির বিরুদ্ধে আন্দোলন শুরু করে।', 'extracted_names': 'কোন নাম খুঁজে পাওয়া যায় নি/No name is found'}\n",
            "{'sentence': 'ডা. মো. শরিফুল ইসলাম, শহীদ সোহরাওয়ার্দী মেডিকেল, কলেজ ও হাসপাতাল।', 'extracted_names': [{'name': 'ডা. মো. শরিফুল ইসলাম', 'label': 'PER', 'start': 0, 'end': 4}]}\n"
          ]
        }
      ]
    }
  ]
}